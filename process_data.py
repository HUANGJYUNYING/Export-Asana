import os
import json
import glob
import sys
import re
from asana import Configuration, ApiClient

import config
import utils
import markdown_render
import llm_processor

def protect_asana_links(text):
    """
    å°‡ Asana é™„ä»¶é€£çµæ›¿æ›ç‚ºç‰¹æ®Š Placeholderï¼Œé¿å…è¢« LLM é®ç½©æˆ [LINK]
    Target: https://app.asana.com/.../get_asset?asset_id=123456
    Result: <<<ASSET_123456>>>
    """
    if not text:
        return text
    # Regex æŠ“å– asset_id
    pattern = r"https://app\.asana\.com/[^\s]*asset_id=(\d+)"
    return re.sub(pattern, r"<<<ASSET_\1>>>", text)


def restore_asana_links(text):
    """
    å°‡ Placeholder é‚„åŸå›åŸå§‹é€£çµï¼Œä»¥ä¾¿ markdown_render é€²è¡Œè™•ç†
    Target: <<<ASSET_123456>>>
    Result: https://app.asana.com/app/asana/-/get_asset?asset_id=123456
    """
    if not text:
        return text
    # é‚„åŸå›æ¨™æº– Asana Asset URL æ ¼å¼ (é€™æ ¼å¼æ˜¯å›ºå®šçš„)
    return re.sub(
        r"<<<ASSET_(\d+)>>>",
        r"https://app.asana.com/app/asana/-/get_asset?asset_id=\1",
        text,
    )


def collect_texts_to_mask(data):
    """
    å¾ JSON è³‡æ–™ä¸­éè¿´æ”¶é›†æ‰€æœ‰éœ€è¦é®ç½©çš„å­—ä¸²
    """
    texts = set()
    t = data["metadata"]

    # 1. ä¸»ä»»å‹™
    if t.get("name"):
        texts.add(t["name"])
    if t.get("notes"):
        texts.add(protect_asana_links(t["notes"]))
    # è‡ªè¨‚æ¬„ä½
    if t.get("custom_fields"):
        for cf in t["custom_fields"]:
            if cf.get("display_value"):
                texts.add(cf["display_value"])

    # 2. é™„ä»¶åç¨±
    if data.get("task_attachments"):
        for a in data["task_attachments"]:
            if a.get("name"):
                texts.add(a["name"])
            if a.get("ocr_text"):
                texts.add(a["ocr_text"])

    # 3. ç•™è¨€
    if data.get("stories"):
        for s in data["stories"]:
            if s.get("text"):
                texts.add(protect_asana_links(s["text"]))
            user_name = (s.get("created_by") or {}).get("name")
            if user_name:
                texts.add(user_name)

    # 4. å­ä»»å‹™ (éè¿´æ¦‚å¿µ)
    if data.get("subtasks"):
        for sub in data["subtasks"]:
            sm = sub["meta"]
            if sm.get("name"):
                texts.add(sm["name"])
            if sm.get("notes"):
                texts.add(protect_asana_links(sm["notes"]))

            # å­ä»»å‹™é™„ä»¶
            if sub.get("attachments"):
                for sa in sub["attachments"]:
                    if sa.get("name"):
                        texts.add(sa["name"])
                    if sa.get("ocr_text"):
                        texts.add(sa["ocr_text"])

            # å­ä»»å‹™ç•™è¨€
            if sub.get("stories"):
                for ss in sub["stories"]:
                    if ss.get("text"):
                        texts.add(protect_asana_links(ss["text"]))
                    sub_user_name = (ss.get("created_by") or {}).get("name")
                    if sub_user_name:
                        texts.add(sub_user_name)

    return list(texts)


def run_process(target_proj_name=None):
    if not os.path.exists(config.RAW_DIR):
        print("âŒ æ‰¾ä¸åˆ°åŸå§‹è³‡æ–™")
        return

    # é¸æ“‡å°ˆæ¡ˆ
    if target_proj_name:
        target_proj = target_proj_name
    else:
        projects = [
            d
            for d in os.listdir(config.RAW_DIR)
            if os.path.isdir(os.path.join(config.RAW_DIR, d))
        ]
        if not projects:
            print("âŒ ç„¡å°ˆæ¡ˆè³‡æ–™")
            return
        print("\nğŸ“‹ è³‡æ–™è™•ç†èˆ‡ç”Ÿæˆ")
        for i, p in enumerate(projects):
            print(f"  {i+1}) {p}")
        try:
            idx = int(input("ğŸ‘‰ ç·¨è™Ÿï¼š")) - 1
            target_proj = projects[idx]
        except:
            return

    json_path = os.path.join(config.RAW_DIR, target_proj, "json_tasks")
    output_proj_path = os.path.join(config.PROCESSED_DIR, target_proj)

    # æº–å‚™ API (ç”¨æ–¼é è¦½)
    profiles = config.load_asana_profiles()
    token = profiles[0]["token"]
    conf = Configuration()
    conf.access_token = token
    client = ApiClient(configuration=conf)

    files = glob.glob(os.path.join(json_path, "*.json"))
    print(f"\nğŸš€ [Stage 2] é–‹å§‹è™•ç† {len(files)} å€‹æª”æ¡ˆ...")
    print(f"ğŸ”’ é®ç½©: {'True' if config.ENABLE_LLM_ANALYSIS else 'False'}")

    for i, fpath in enumerate(files):
        sys.stdout.write(f"\r   é€²åº¦: {i+1}/{len(files)}...")
        sys.stdout.flush()

        with open(fpath, "r", encoding="utf-8") as f:
            data = json.load(f)

        t = data["metadata"]

        # æ‰¹æ¬¡é®ç½© (Batch Masking)
        mask_lookup = {}

        if config.ENABLE_LLM_ANALYSIS:
            # 1. æ”¶é›†æ‰€æœ‰å­—ä¸²
            all_texts = collect_texts_to_mask(data)

            # 2. ä¸€æ¬¡æ€§é€çµ¦ LLM(è®“llm_processor å…§éƒ¨è‡ªå‹•åˆ†æ‰¹è™•ç†ä»¥ç¬¦åˆ token é™åˆ¶, LLM æœƒçœ‹åˆ° <<<ASSET_123>>> ä¸¦ä¿ç•™å®ƒ)
            mask_lookup = llm_processor.mask_batch_texts(all_texts)

        # 3. å®šç¾©å¿«é€ŸæŸ¥æ‰¾å‡½å¼
        def _mask(txt):
            if not txt:
                return ""
            if not config.ENABLE_LLM_ANALYSIS:
                return txt

            # A. å…ˆä¿è­·å‚³å…¥çš„æ–‡å­— (å› ç‚º lookup key æ˜¯ä¿è­·éçš„)
            protected_txt = protect_asana_links(txt)

            # B. æŸ¥è¡¨å–å¾—é®ç½©å¾Œçµæœ
            masked_txt = mask_lookup.get(protected_txt, protected_txt)

            # C. é‚„åŸé€£çµ (è®“ markdown_render èƒ½è®€åˆ° ID)
            final_txt = restore_asana_links(masked_txt)

            return final_txt

        # æ¸²æŸ“èˆ‡å­˜æª”
        md_lines = markdown_render.render_markdown(data, _mask)

        # Raw Data ç›¸å°è·¯å¾‘
        final_md_lines = []
        path_prefix = f"../../../raw_data/{target_proj}/attachments/"
        for line in md_lines:
            line = line.replace("../attachments/", path_prefix)
            final_md_lines.append(line)

        final_md_content = "\n".join(final_md_lines)
        # å­˜æª”
        sec_dir = os.path.join(output_proj_path, data["section_name"])
        os.makedirs(sec_dir, exist_ok=True)

        safe_title = _mask(t["name"])
        c_at = t["created_at"][:10].replace("-", "")
        fname = f"{c_at}_{utils.clean_filename(safe_title)}.md"
        if len(fname) > 100:
            fname = fname[:100] + ".md"

        with open(os.path.join(sec_dir, fname), "w", encoding="utf-8") as f:
            f.write("\n".join(final_md_lines))

        # å¯«å›é è¦½
        if config.ENABLE_LLM_ANALYSIS and os.getenv("ENABLE_UPLOAD_PREVIEW") == "True":
            preview_stories = []
            for s in data["stories"]:
                if s["resource_subtype"] == "comment_added":
                    u = _mask(s.get("created_by", {}).get("name", "User"))
                    txt = _mask(s["text"])
                    preview_stories.append(f"{u}: {txt}")

            utils.post_masking_preview(
                client, t["gid"], final_md_content
            )  # ç›´æ¥å‚³é€æª”æ¡ˆå…§å®¹

    print(f"\nâœ… è™•ç†å®Œæˆï¼")


if __name__ == "__main__":
    run_process()
